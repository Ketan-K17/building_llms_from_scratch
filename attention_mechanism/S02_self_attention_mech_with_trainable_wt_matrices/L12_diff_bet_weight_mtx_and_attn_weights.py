# WEIGHT PARAMETERS VS. ATTENTION WEIGHTS

# In the weight matrices W, the term “weight” is short for
# “weight parameters,” the values of a neural network that
# are optimized during training. This is not to be confused
# with the attention weights. As we already saw, attention
# weights determine the extent to which a context vector
# depends on the different parts of the input (i.e., to what
# extent the network focuses on different parts of the input).
# In summary, weight parameters are the fundamental,
# learned coefficients that define the network’s connections,
# while attention weights are dynamic, context-specific
# values.